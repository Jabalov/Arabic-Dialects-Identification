{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0136a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, json, render_template\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import nltk \n",
    "# nltk.download('all')\n",
    "from nltk.corpus import stopwords, wordnet \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53025895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb131cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a300f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5beab66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClassificationModel',\n",
       " 'Datasets',\n",
       " 'env.yml',\n",
       " 'Machine Learning Task.pdf',\n",
       " 'MARBERT',\n",
       " 'model',\n",
       " 'Notebooks',\n",
       " 'requirements.txt',\n",
       " 'Scripts',\n",
       " 'templates',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8cb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"Scripts\")\n",
    "sys.path.append(\"dialects\")\n",
    "sys.path.append(\"templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a676a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_and_tokenize import *\n",
    "from dialects import dialects_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1878c",
   "metadata": {},
   "source": [
    "---\n",
    "## Models Loading (Tokenizers for MARBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf97673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mohab\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohab\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohab\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator NearestNeighbors from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohab\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LinearSVC from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "marebert_model = TFAutoModelForSequenceClassification.from_pretrained(\"UBC-NLP/MARBERT\", num_labels=18) \n",
    "\n",
    "marebert_model.load_weights(\"MARBERT/tf_model.h5\")\n",
    "svm_pipeline = joblib.load(\"ClassificationModel/classification_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60007d51",
   "metadata": {},
   "source": [
    "## Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57cc7cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [13/Mar/2022 14:54:56] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2022 14:55:23] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2022 14:55:43] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2022 14:56:33] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2022 14:58:20] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2022 14:59:43] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2022 14:59:57] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/prediction\", methods=[\"POST\"])\n",
    "def prediction():\n",
    "    \n",
    "    # Request parsing and preprocessing\n",
    "    text_inp = list(request.form.values())[0]\n",
    "    preprocessed_text = PreprocessTweets(text_inp).preprocessing_pipeline()\n",
    "    \n",
    "    # Tokenization is done twice, one for svm and other for marbert\n",
    "    tokenized_for_ml = TweetsTokenizing(preprocessed_text).tokenize_pipeline()\n",
    "    tokenized_for_bert = tokenizer.encode(preprocessed_text, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "    \n",
    "    # Do prediction using two models\n",
    "    ml_predict = f\"SVM Prediction: {svm_pipeline.predict([f'{tokenized_for_ml}'])[0]}\"\n",
    "    bert_predict = f\"MARBERT Prediction: {dialects_dict[np.argmax(marebert_model.predict(tokenized_for_bert)[0], axis=1)[0]]}\"\n",
    "    \n",
    "    # Rendering with predictions\n",
    "    return render_template(\"index.html\", prediction_ml=ml_predict, prediction_dl=bert_predict)\n",
    "\n",
    "app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91dc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
